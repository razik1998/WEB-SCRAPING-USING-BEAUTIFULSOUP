{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356e7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "#   days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#   Scrape below mentioned details and make data frame\u0002\n",
    "# i) Paper Title\n",
    "# ii) Authors\n",
    "# iii) Published Date\n",
    "# iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0efe2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Importing Required Labraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13f69e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Send Request To The Web_page Server\n",
    "\n",
    "page = requests.get ('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ac5bca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1843311724.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_15008\\1843311724.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    soup.8 = BeautifulSoup (page.content)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#  Only Need Particular ContentS From Web_Site\n",
    "\n",
    "soup.8 = BeautifulSoup (page.content)\n",
    "soup.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3080de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Scraping All Paper Title In One Go\n",
    "\n",
    "Papers = []\n",
    "\n",
    "for i in soup.find_all ('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "    Papers.append (i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379121b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reward is enough',\n",
       " 'Making sense of raw input',\n",
       " 'Law and logic: A review from an argumentation perspective',\n",
       " 'Creativity and artificial intelligence',\n",
       " 'Artificial cognition for social human–robot interaction: An implementation',\n",
       " 'Explanation in artificial intelligence: Insights from the social sciences',\n",
       " 'Making sense of sensory input',\n",
       " 'Conflict-based search for optimal multi-agent pathfinding',\n",
       " 'Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning',\n",
       " 'The Hanabi challenge: A new frontier for AI research',\n",
       " 'Evaluating XAI: A comparison of rule-based and example-based explanations',\n",
       " 'Argumentation in artificial intelligence',\n",
       " 'Algorithms for computing strategies in two-player simultaneous move games',\n",
       " 'Multiple object tracking: A literature review',\n",
       " 'Selection of relevant features and examples in machine learning',\n",
       " 'A survey of inverse reinforcement learning: Challenges, methods and progress',\n",
       " 'Explaining individual predictions when features are dependent: More accurate approximations to Shapley values',\n",
       " 'A review of possible effects of cognitive biases on interpretation of rule-based machine learning models',\n",
       " 'Integrating social power into the decision-making of cognitive agents',\n",
       " \"“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems\",\n",
       " 'Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies',\n",
       " 'Algorithm runtime prediction: Methods & evaluation',\n",
       " 'Wrappers for feature subset selection',\n",
       " 'Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics',\n",
       " 'Quantum computation, quantum theory and AI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying All Paper Titels List\n",
    "\n",
    "Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "434c6aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Scraping All Paper Authors In One Go\n",
    "\n",
    "Authors = []\n",
    "\n",
    "for i in soup.find_all ('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "    Authors.append (i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d75fba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. ',\n",
       " 'Evans, Richard, Bošnjak, Matko and 5 more',\n",
       " 'Prakken, Henry, Sartor, Giovanni ',\n",
       " 'Boden, Margaret A. ',\n",
       " 'Lemaignan, Séverin, Warnier, Mathieu and 3 more',\n",
       " 'Miller, Tim ',\n",
       " 'Evans, Richard, Hernández-Orallo, José and 3 more',\n",
       " 'Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. ',\n",
       " 'Sutton, Richard S., Precup, Doina, Singh, Satinder ',\n",
       " 'Bard, Nolan, Foerster, Jakob N. and 13 more',\n",
       " 'van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark ',\n",
       " 'Bench-Capon, T.J.M., Dunne, Paul E. ',\n",
       " 'Bošanský, Branislav, Lisý, Viliam and 3 more',\n",
       " 'Luo, Wenhan, Xing, Junliang and 4 more',\n",
       " 'Blum, Avrim L., Langley, Pat ',\n",
       " 'Arora, Saurabh, Doshi, Prashant ',\n",
       " 'Aas, Kjersti, Jullum, Martin, Løland, Anders ',\n",
       " 'Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes ',\n",
       " 'Pereira, Gonçalo, Prada, Rui, Santos, Pedro A. ',\n",
       " 'Riveiro, Maria, Thill, Serge ',\n",
       " 'Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. ',\n",
       " 'Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin ',\n",
       " 'Kohavi, Ron, John, George H. ',\n",
       " 'Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna ',\n",
       " 'Ying, Mingsheng ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying All Authors Names List\n",
    "\n",
    "Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15afd535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Scraping All Paper Published Date In One Go\n",
    "\n",
    "Dates = []\n",
    "\n",
    "for i in soup.find_all ('span',class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "    Dates.append (i.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9645e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['October 2021',\n",
       " 'October 2021',\n",
       " 'October 2015',\n",
       " 'August 1998',\n",
       " 'June 2017',\n",
       " 'February 2019',\n",
       " 'April 2021',\n",
       " 'February 2015',\n",
       " 'August 1999',\n",
       " 'March 2020',\n",
       " 'February 2021',\n",
       " 'October 2007',\n",
       " 'August 2016',\n",
       " 'April 2021',\n",
       " 'December 1997',\n",
       " 'August 2021',\n",
       " 'September 2021',\n",
       " 'June 2021',\n",
       " 'December 2016',\n",
       " 'September 2021',\n",
       " 'May 2021',\n",
       " 'January 2014',\n",
       " 'December 1997',\n",
       " 'October 2021',\n",
       " 'February 2010']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying All Paper Published Date List\n",
    "\n",
    "Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a4e6377",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15008\\3314888847.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mUrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sc-5smygv-0 nrDZj\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mUrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"href\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup8' is not defined"
     ]
    }
   ],
   "source": [
    "#  Scraping All  Paper Url In One Go\n",
    "\n",
    "Url = []\n",
    "\n",
    "for i in soup8.find_all ('a',class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    Url.append (i.get(\"href\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ecf8798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying All Url Links\n",
    "Url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3d60782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 0\n"
     ]
    }
   ],
   "source": [
    "print (len(Papers),len(Authors),len(Dates),len(Url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ada79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Data Frame & Displaying Data In Data_frame\n",
    "\n",
    "df = pd.DataFrame ({'Paper Title':Papers,'Authors':Authors,'Published Date':Dates,'Paper Url':Url})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
